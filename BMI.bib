
@book{matloff_statistical_2017,
	address = {Boca Raton},
	edition = {1 edition},
	title = {Statistical {Regression} and {Classification}: {From} {Linear} {Models} to {Machine} {Learning}},
	isbn = {978-1-4987-1091-6},
	shorttitle = {Statistical {Regression} and {Classification}},
	abstract = {Statistical Regression and Classification: From Linear Models to Machine Learning takes an innovative look at the traditional statistical regression course, presenting a contemporary treatment in line with today's applications and users. The text takes a modern look at regression: * A thorough treatment of classical linear and generalized linear models, supplemented with introductory material on machine learning methods. * Since classification is the focus of many contemporary applications, the book covers this topic in detail, especially the multiclass case. * In view of the voluminous nature of many modern datasets, there is a chapter on Big Data. * Has special Mathematical and Computational Complements sections at ends of chapters, and exercises are partitioned into Data, Math and Complements problems. * Instructors can tailor coverage for specific audiences such as majors in Statistics, Computer Science, or Economics. * More than 75 examples using real data.  The book treats classical regression methods in an innovative, contemporary manner. Though some statistical learning methods are introduced, the primary methodology used is linear and generalized linear parametric models, covering both the Description and Prediction goals of regression methods. The author is just as interested in Description applications of regression, such as measuring the gender wage gap in Silicon Valley, as in forecasting tomorrow's demand for bike rentals. An entire chapter is devoted to measuring such effects, including discussion of Simpson's Paradox, multiple inference, and causation issues. Similarly, there is an entire chapter of parametric model fit, making use of both residual analysis and assessment via nonparametric analysis.  Norman Matloff is a professor of computer science at the University of California, Davis, and was a founder of the Statistics Department at that institution. His current research focus is on recommender systems, and applications of regression methods to small area estimation and bias reduction in observational studies. He is on the editorial boards of the Journal of Statistical Computation and the R Journal. An award-winning teacher, he is the author of The Art of R Programming and Parallel Computation in Data Science: With Examples in R, C++ and CUDA.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Matloff, Norman},
	month = aug,
	year = {2017}
}

@article{johnson_fitting_1996,
	title = {Fitting {Percentage} of {Body} {Fat} to {Simple} {Body} {Measurements}},
	volume = {4},
	url = {https://amstat-tandfonline-com.proxy006.nclive.org/doi/abs/10.1080/10691898.1996.11910505},
	doi = {10.1080/10691898.1996.11910505},
	abstract = {Percentage of body fat, age, weight, height, and ten body circumference measurements (e.g., abdomen) are recorded for 252 men. Body fat, one measure of health, has been accurately estimated by an underwater weighing technique. Fitting body fat to the other measurements using multiple regression provides a convenient way of estimating body fat for men using only a scale and a measuring tape. This dataset can be used to show students the utility of multiple regression and to provide practice in model building.},
	number = {1},
	urldate = {2018-04-05},
	journal = {Journal of Statistics Education},
	author = {Johnson, Roger W.},
	month = mar,
	year = {1996},
	file = {Snapshot:/Users/alan/Zotero/storage/K6TZS4G4/10691898.1996.html:text/html}
}

@article{olive_visualizing_2015,
	title = {Visualizing and {Testing} the {Multivariate} {Linear} {Regression} {Model}},
	volume = {4},
	copyright = {Copyright (c)},
	issn = {1927-7040},
	url = {http://www.ccsenet.org/journal/index.php/ijsp/article/view/43460},
	doi = {10.5539/ijsp.v4n1p126},
	abstract = {Recent results make the multivariate linear regression model much easier to use. This model has \$m {\textbackslash}geq 2\$ response variables. Results by Kakizawa (2009) and Su and Cook (2012) can be used to explain the large sample theory of the least squares estimator and of the widely used Wilks' \${\textbackslash}Lambda\$, Pillai's trace, and Hotelling Lawley trace test statistics. Kakizawa (2009) shows that these statistics have the same limiting distribution. This paper reviews these results and gives two theorems to show that the Hotelling Lawley test generalizes the usual partial \$F\$ test for \$m = 1\$ response variable to \$m {\textbackslash}geq 1\$ response variables. Plots for visualizing the model are also given, and can be used to check goodness and lack of fit, to check for outliers and influential cases, and to check whether the error distribution is multivariate normal or from some other elliptically contoured distribution.},
	language = {en},
	number = {1},
	urldate = {2018-04-06},
	journal = {International Journal of Statistics and Probability},
	author = {Olive, David J. and Pelawa Watagoda, Lasanthi C. R.  and Rupasinghe Arachchige Don, Hasthika S.},
	month = jan,
	year = {2015},
	pages = {126},
	file = {Full Text PDF:/Users/alan/Zotero/storage/AUMM6RQY/Olive et al. - 2015 - Visualizing and Testing the Multivariate Linear Re.pdf:application/pdf;Snapshot:/Users/alan/Zotero/storage/RLDJHTIX/43460.html:text/html}
}

@article{hoerl_ridge_1970,
	title = {Ridge {Regression}: {Biased} {Estimation} for {Nonorthogonal} {Problems}},
	volume = {12},
	issn = {0040-1706},
	shorttitle = {Ridge {Regression}},
	url = {http://www.jstor.org/stable/1267351},
	doi = {10.2307/1267351},
	abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
	number = {1},
	urldate = {2018-04-13},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	year = {1970},
	pages = {55--67}
}

@book{james_introduction_2017,
	address = {New York},
	edition = {1st ed. 2013, Corr. 7th printing 2017 edition},
	title = {An {Introduction} to {Statistical} {Learning}: with {Applications} in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An {Introduction} to {Statistical} {Learning}},
	abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
	language = {English},
	publisher = {Springer},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	month = sep,
	year = {2017}
}

@book{olive_robust_2017,
	address = {New York, NY},
	edition = {1st ed. 2017 edition},
	title = {Robust {Multivariate} {Analysis}},
	isbn = {978-3-319-68251-8},
	abstract = {This text presents methods that are robust to the assumption of a multivariate normal distribution or methods that are robust to certain types of outliers. Instead of using exact theory based on the multivariate normal distribution, the simpler and more applicable large sample theory is given.  The text develops among the first practical robust regression and robust multivariate location and dispersion estimators backed by theory.    The robust techniques  are illustrated for methods such as principal component analysis, canonical correlation analysis, and factor analysis.  A simple way to bootstrap confidence regions is also provided.   Much of the research on robust multivariate analysis in this book is being published for the first time.  The text is suitable for a first course in Multivariate Statistical Analysis or a first course in Robust Statistics.  This graduate text is also useful for people who are familiar with the traditional multivariate topics, but want to know more about handling data sets with outliers. Many R programs and R data sets are available on the author’s website.},
	language = {English},
	publisher = {Springer},
	author = {Olive, David J.},
	month = nov,
	year = {2017}
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	urldate = {2018-04-13},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288}
}

@misc{noauthor_regularization_nodate,
	title = {Regularization and variable selection via the elastic net - {Zou} - 2005 - {Journal} of the {Royal} {Statistical} {Society}: {Series} {B} ({Statistical} {Methodology}) - {Wiley} {Online} {Library}},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x},
	urldate = {2018-04-13}
}

@article{zou_regularization_2005,
	title = {Regularization and variable selection via the elastic net},
	volume = {67},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x},
	doi = {10.1111/j.1467-9868.2005.00503.x},
	language = {en},
	number = {2},
	urldate = {2018-04-13},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Zou, Hui and Hastie, Trevor},
	month = apr,
	year = {2005},
	pages = {301--320},
	file = {Full Text PDF:/Users/alan/Zotero/storage/C3W6Q4KP/Zou and Hastie - 2005 - Regularization and variable selection via the elas.pdf:application/pdf;Snapshot:/Users/alan/Zotero/storage/IYKT2HPM/j.1467-9868.2005.00503.html:text/html}
}

@book{hastie_elements_2009,
	edition = {2 edition},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}, {Second} {Edition}},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book.This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates.},
	language = {English},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	month = aug,
	year = {2009}
}

@book{hastie_statistical_2015,
	address = {Boca Raton},
	edition = {1 edition},
	title = {Statistical {Learning} with {Sparsity}: {The} {Lasso} and {Generalizations}},
	isbn = {978-1-4987-1216-3},
	shorttitle = {Statistical {Learning} with {Sparsity}},
	abstract = {Discover New Methods for Dealing with High-Dimensional Data  A sparse statistical model has only a small number of nonzero parameters or weights; therefore, it is much easier to estimate and interpret than a dense model. Statistical Learning with Sparsity: The Lasso and Generalizations presents methods that exploit sparsity to help recover the underlying signal in a set of data.  Top experts in this rapidly evolving field, the authors describe the lasso for linear regression and a simple coordinate descent algorithm for its computation. They discuss the application of ℓ1 penalties to generalized linear models and support vector machines, cover generalized penalties such as the elastic net and group lasso, and review numerical methods for optimization. They also present statistical inference methods for fitted (lasso) models, including the bootstrap, Bayesian methods, and recently developed approaches. In addition, the book examines matrix decomposition, sparse multivariate analysis, graphical models, and compressed sensing. It concludes with a survey of theoretical results for the lasso.  In this age of big data, the number of features measured on a person or object can be large and might be larger than the number of observations. This book shows how the sparsity assumption allows us to tackle these problems and extract useful and reproducible patterns from big datasets. Data analysts, computer scientists, and theorists will appreciate this thorough and up-to-date treatment of sparse statistical modeling.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	month = may,
	year = {2015}
}